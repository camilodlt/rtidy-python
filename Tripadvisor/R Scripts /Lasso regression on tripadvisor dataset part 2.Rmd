---
title: "Lasso regresssion on TripAdvisor dataset Part 2"
author: "Camilo De La Torre"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE,message = FALSE)
```

#### Packages 

This time we will be using Spacyr package to tokenize the reviews. The difference between tokenizers and Spacyr is that with Spacyr we can then use step_lemma that performs lemmatization.  
You can take a look at the [vignette here](https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html) or the [documentation here](https://github.com/quanteda/spacyr).  *

Notice: 
- to install and use SpaCy you need to have Python installed in your computer.  
- You need to install SpaCy before using it. You can achieve this using the spacy_install() command that will install SpaCy in your conda environment.  

```{r,message=FALSE}
if(!require("tidytext")){install.packages("tidytext")}
if(!require("magrittr")){install.packages("magrittr")}
if(!require("tidyverse")){install.packages("tidyverse")}
if(!require("tidymodels")){install.packages("tidymodels")}
if(!require("textrecipes")){install.packages("textrecipes")}
if(!require("stopwords")){install.packages("stopwords")}
if(!require("spacyr")){install.packages("spacyr")}
if(!require("vip")){install.packages("vip")}
if(!require("doParallel")){install.packages("doParallel")}

library(tidytext)
library(magrittr)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(stopwords)
library("spacyr")
library(vip)
library(doParallel)

# Installing Spacyr for the first time 
# spacy_install()
# spacy_download_langmodel("en_core_web_lg")

# Initialize SpaCyr with the large english model 
spacy_initialize(model="en_core_web_lg")
```

### Importing  
As usual we import the tripAdvisor dataframe.  
```{r}
setwd("~/Documents")
tripadvisor<-read.csv(file="Blog/rtidy-python/Tripadvisor/Datasets/Kaggle/tripadvisor_hotel_reviews.csv",
         header = TRUE, 
         sep = ",", # the default
         encoding = "UTF-8") 
```

### Some cleaning of the data  
We remove some unicode characters that won't be used as tokens.  

```{r}
tripadvisor %<>%
  dplyr::mutate(Review=iconv(Review,'utf-8', 'ascii', sub=''), # Remove the unicode characters
                Review=str_remove_all(Review, pattern = "_*")) # remove the substitutes for the characters removed

```

### Initial split  
Let s keep 80% for training and 20 for testing.
That gives us 16395 rows for training and 4096 for testing our model. 

```{r}
set.seed(1234) # we set the seed to have reproducible results 
tripadvisor_split <- tripadvisor%>%
  initial_split(prop=0.8, strata = Rating) # we use the initial_splits who stores the split for us 

tripadvisor_train <- training(tripadvisor_split) # we separate the split for training
tripadvisor_test <- testing(tripadvisor_split) # and testing 
```

### Lasso regression  

We define our model. We are tuning a Lasso model and tuning its regularization parameter.  
```{r}
tune_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>% # we set the classification mode 
  set_engine("glmnet") # using gmlnet for the computation 
```

### Recipe  

Now let's create the recipe (i.e the steps that we are going to take in order to preprocess the data).  
We are trying to predict the hotels' ratings by the content of the reviews. So, we use a formula of the type : `r Rating ~ Review`.  

Several steps are taken: 
- We tokenize with SpaCyr (in order to do the lemmatization)  
- We then remove stopwords that are included in the snowball dictionnary. 
- Then, we lemmatize the tokens that were not removed.=> this uses the spacyr package in the back.  
- We tune the parameter of the maximum number of tokens to be used.  
- We calculate the TFIDF score for each token left and for each Review. 
- Lastly, we scale all the columns (except for the Rating since it's not a predictor, each token is now a column, the values in each cell are the tfidf scores) because the Lasso regression that we define in the model step requires us to do so.  

## Demonstrate the purpose of lemmatization  

Lemmatization groups together the multiple [inflected](https://en.wikipedia.org/wiki/Inflection) words into the original [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)).  

The idea is that words providing the same meaning (but in different forms) are grouped so that they are later evaluated only by one metric (tfidf for example). We would prefer to have only one tfidf for 'do', 'did','does'. 

Let's say we have the phrase: "Didn't enjoy our stay at the Hotel. I expected more. Food did not arrive at the good temperature. When we arrived, nobody welcomed us. I expect that they improve their standards soon."  
```{r}
example_phrase<- "Didn't enjoy our stay at the Hotel. I expected more. Food did not arrive at the good temperature. When we arrived, nobody welcomed us. I expect that they improve their standards soon."

spacyr::spacy_parse(example_phrase)%>% 
  pull(lemma) %>% paste(collapse = " ")
```
As we can see, *did* was transformed to *do*, *n't* to *not*, *expected* to *expect*, *arrived* to *arrive*, *welcomed* to *welcome* and so on. Also, *Our*,*I*,*we* were grouped into *PRON*.  

Last time we removed nltk stopwords. This time we will remove the stopwords from [*marimo*](https://github.com/koheiw/marimo). It is an extension from snowball but the really interesting thing is that you can opt-out from some categories. To that matter, we will use all the words in english except the adverbs because I think that words like "not", "few", "little" could be useful.  

We can implement the lemmatization in our pipeline with step_lemma(). 

This time we also tune the maximum number of tokens to be kept for the model.  

```{r recipe}
# Recipe 

tripadvisor_recipe <- recipe(Rating ~ Review, data = tripadvisor_train%>% head()) %>%
  step_mutate(Rating=factor(Rating, levels = c("5","4","3","2","1")))%>%
  step_tokenize(Review, token = "words", engine="spacyr") %>% # lowers also characters. SpaCyr just support token="words"
  step_stopwords(Review, custom_stopword_source = unlist(stopwords::data_stopwords_marimo$en[-5], use.names = F))%>% 
  step_lemma(Review)%>% 
  step_tokenfilter(Review, max_tokens = tune()) %>%
  step_tfidf(Review) %>%
  step_normalize(all_predictors())

```

### The workflow   

We add our recipe and our model to a worflow.  
Let's take a look at the workflow to have a summary of what we are building. 
```{r}
tripadvisor_workflow <- workflow() %>% # initialize the workflow
  add_recipe(tripadvisor_recipe)%>% # We add the recipe that we created (with the steps)
  add_model(tune_spec) # We add the model that we specified (regression with glmnet)

# Let's print it  
tripadvisor_workflow
```

### Tuning the parameters.  

Now we tune all the parameters that we defined. We have to provide values to try for penalty and max_tokens.  We create 10 values for each (10*10 combinations= 100 models)
```{r}
final_grid <- grid_regular(
  penalty(range = c(-4, 0)),
  max_tokens(c(50,1000)),
  levels = c(penalty = 10)
)

# Cross validation. 
# Train models on V-1 models and assess the model on the partition that wasn't used. 
tripadvisor_folds <- vfold_cv(tripadvisor_train, v=10, repeats = 3)

# The foreach package is used by the tune package so we can use multiple processors to speed up the process. 

# Do parallel 
all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores,setup_strategy = "sequential")
registerDoParallel(cl)

# Tune package 
final_rs <- tune_grid(
  tripadvisor_workflow,# We pass the workflow 
  tripadvisor_folds,# the 10 folds
  grid = final_grid,# The grid for the penalty values to be used
  metrics = metric_set(accuracy, roc_auc),# the metrics that we want to follow
  control = control_grid(save_pred = F,pkgs = c("textrecipes"))# We save the predictions made at every fold trained
)

tuned<-final_rs %>%select_best("accuracy")# We select the penalty that yields the best accuracy 
```

Let's take a look at the best parameters found:  
```{r}
tuned
```
We can see the results that we obtained with this specification :

```{r}
final_rs$.metrics%>% map_dfr(~filter(.x,penalty==tuned$penalty, max_tokens==tuned$max_tokens ))%>% group_by(.metric)%>% summarise(estimate=mean(.estimate))
```

### Visualizing the cross validation  

```{r}
final_rs %>%
    collect_metrics() %>% # handy function to show the mean metrics for each penalty 
  # Plot 
    ggplot(aes(penalty, mean, color=max_tokens)) + 
    geom_line(size = 1.5, alpha = 0.5) +# to connect the dots 
    geom_point(size = 2, alpha = 0.9) +
    facet_wrap(~.metric, scales = "free_y") + 
    scale_color_viridis_c()+
    scale_x_log10() +
    labs(
        title = "Metrics by associated penalty values for Lasso regression"
    )
```
We can see that :   
- More tokens is better up to a point.  
- Both metrics fall rapidly after the penalty keeps increasing over 0.01.  
- The best results are obtained with an small penalty parameter and a high number of tokens.  

### Finalyze a workflow  
In order to finalize a workflow, we pass the best tuned parameters that were given by the select_best function. Models will then be trained using the parameters that we found.    
```{r}
final_wf <- finalize_workflow(
  tripadvisor_workflow,# our previous workflow
  tuned # the best parameter
)

```

### Last fit and important predictors  

We perform a fit to the training data to gain more insight. And then we test our results with the testing set.  
```{r}
# Last Fit 
final_fit <- final_wf %>% # we use our finalyzed workflow 
  fit(tripadvisor_train) %>% # we fit the training data 
  pull_workflow_fit()# we substract the result from glmnet 

final_fit_vi<-final_fit%>% vi(lambda = tuned$penalty)# and use the vip package to get the estimators and their sign (positive or negative)

# Let's visualize the 50 most important predictors (25 positives and 25 negatives)
final_fit_vi %>%
  mutate(
    Importance = abs(Importance), # so every bar goes to the right
    Variable = str_remove_all(Variable, "tfidf_Review_")# we remove the preffix from the columns, this preffix was given in the step_tfidf from the recipe
  ) %>%
  group_by(Sign) %>%# group by sign for facetting
  top_n(25, Importance) %>% # select the top 25 with dplyr
  ungroup() %>%
  # Plot 
  ggplot(aes(
    x = Importance,
    y = fct_reorder(Variable, Importance),
    fill = Sign
  )) +
  geom_col(show.legend = FALSE) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_wrap(~Sign, scales = "free") +
  labs(
    y = NULL,
    title = "Variable importance"
  )
```
Excellent, great, fantastic, perfect, wonderful... are words that increase the chances of the reviews (and the hotels) to have a high rating. On the contrary, bad, poor, rude, dirty, ok ... are associated with low ratings.  

We can see that customers complained about the loudness, the location, the space and the smell. Happy customers were probably satisfied with the opposite experiences ("spacious", quiet, spotless, decorate..)

Now that we know this, let's run the last fit to see how our model behaves with data it has never seen. We can see the metric:
```{r}
final_fitted <- last_fit(
  final_wf,# the finalyzed workflow
  tripadvisor_split # The initial split 
)
# Collecting the metrics 
final_fitted %>%
  collect_metrics()

save.image(file="Spacyr.RData")

```

We obtain 0.608 accuracy on the testing data (0.59 on training) and a ROC AUC of 0.843.  

Let's see the confusion matrix:  

```{r, dpi=300}
final_fitted %>%
     collect_predictions()%>% conf_mat(truth=Rating, estimate=.pred_class)%>% autoplot("heatmap")
```

Our model performs better on 5 and 1 stars ratings than on 2,3 and 4. 
```{r}
final_fitted %>%
     collect_predictions() %>% group_by(.pred_class)%>%
     accuracy(Rating, .pred_class)
```

