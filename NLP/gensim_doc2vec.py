# -*- coding: utf-8 -*-
"""GENSIM DOC2VEC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10hrRwNmcQTfamiUkG8Pw5zl3YowcV0gv

## TRY DOC TO VEC ON SOME TWEETS 
---

In the end, results aren't really good, mostly because input data is short (small phrases) and not numerous (in number of training samples).
"""

import pandas as pd

#Load the dataset and explore.
try:
    from google.colab import files
    !wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv
    !ls -lah DATAPATH
    filepath = "DATAPATH/train_data.csv"
except ModuleNotFoundError:
    filepath = "Data/Sentiment and Emotion in Text/train_data.csv"

# READ CSV ------
df = pd.read_csv(filepath)
print(df.shape)
print(df.head())
print(df.tail())

"""The data comes from https://www.kaggle.com/c/sa-emotions/data. 

The database has several sentiments that should be infer by the content of tweet text. 
The dataset is not balanced. 
"""

# SENTIMENTS ------
print(df['sentiment'].value_counts())
print("Number of sentiments in target:", len(df['sentiment'].value_counts()))

"""Although there are a lot of sentiments (13).  We subset the data to only retain 3 of them: neutral, happiness, worry. These 3 categories are well represented in the data set.

Happiness is the the most common positive feeling. 

Worry is the most common negative feeling.
"""

# SUBSET SENTIMENTS ------
#Let us take the top 3 categories and leave out the rest.
shortlist = ['neutral', "happiness", "worry"]
df_subset = df[df['sentiment'].isin(shortlist)]
# Which gives us this shape
df_subset.shape

"""## Treat Tweets as documents
This time we will infer the emotion thanks to the whole tweet, by infering the document embedding

### Tokenize the tweets!
"""

# We should import some nlp libraries ---
import nltk 
import spacy, re
from sklearn.model_selection import train_test_split
# DL NLP LIB ---
!pip install --upgrade gensim
import gensim
print(gensim.__version__)
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec

# Stopwords ---
nltk.download('stopwords') # Stopwords to remove
stop_words = [w.lower() for w in nltk.corpus.stopwords.words()]
# Initiate spacy ---
nlp = spacy.load('en')
# Tokenize tweets FUN ---
  # I found a clean function in stackoverflow. 
  # Does the job well
def clean(input_string):
    """ Sanitize one string """

    # normalize to lowercase 
    string = input_string.lower()

    # spacy tokenizer 
    string_split = [token.text for token in nlp(string)]

    # in case the string is empty 
    if not string_split:
        return '' 

    # remove user 
    # assuming user is the first word and contains an @
    if '@' in string_split[0]:
        del string_split[0]

    # join back to string 
    #string = ' '.join(string_split)

    #remove # and @
    def special(string): 
      for punc in '@#':
        return(string.replace(punc, ''))
    
    list_string= [special(i) for i in string_split]

    # remove 't.co/' links
    list_string = [re.sub(r't.co\/[^\s]+', '', i, flags=re.MULTILINE) for i in list_string]

    # removing stop words 
    list_string = [w for w in list_string if w not in stop_words]

    return list_string

"""We know apply the function to each row. Each document is tokenized and also preprocessed (spaces, hashtags, stopwords...).

This take a little while. 
"""

# APLLY FUN TO CLEAN DOCUMENTS ------
df_subset["content"]=df_subset.content.apply(clean)

"""Know we can separate our data into training and testing sets. Kaggle has a testing set but since we subset only a fraction of the sentiments, it's possible that we won't see good results. Although we should be able to distinguish negative, positive and neutral overall. """

# SEPARATE DATA -----
#df_subset contains only the three categories we chose. 
mydata = df_subset['content']
mycats = df_subset['sentiment']
# TRAIN TEST SPLIT ------
train_data, test_data, train_cats, test_cats = train_test_split(mydata,mycats,random_state=1234)

"""Gensim's Doc2vec documentation explains that each document should be of class TaggedDocument. That is, every document has to have 2 parts: text and tags. It is common that the tag is just an integer index that increases by one step each row. """

# DOC2VEC format ------
#prepare training data in doc2vec format:
train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_data)]

"""Let's take a look: """

# First 5 taggedDocuments ---
train_doc2vec[0:5]

# first 5 sentiments --- 
train_cats[0:5]

"""### Train the unsupervised model  

Know we will fit the model on our documents. 

The embedding space of each document will be 50, with an learning rate of 0.025. This rate will decrease until 0.05.

We remove some common words (in the sense of high frequency) : the stopwrods. But we didn't remove unfrequent words that are less likely to appear in our testing corpus or other documents. We define min_count to 10.  

We also precise dm = 0 to use the PV-DBOW implementation of the algorithm. This algorithm will input at each step the document embedding and the BOW embeddings (concatenation: dm_concat= 1) to predict a centric word. In the end, we get the word embedding and document embeddings. 

Documents that are used in similar context, predicting similar words, concatenated with similar context will be closer in the embedding space.  

We also specify the number of negative sampled documents for the loss function. At each time our word (target) activation will be compared to 20 other words activations with current weights instead of the whole vocabulary. This leads to faster training times.
"""

# I like the output from logging
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
#Train a doc2vec model to learn tweet representations. Use only training data!!
model = Doc2Vec(vector_size=50, alpha=0.025,  min_alpha=0.05, min_count=10, dm =0,dbow_words=1, negative= 20,dm_concat=1, workers=2, epochs=150)
# Build the vocabulary ---
model.build_vocab(train_doc2vec)
# Check BOW method ---
print("DBOW method:", model.dbow)
print("Number of documents:", model.corpus_count)
print("Size of vocab", model.corpus_total_words)

# TRAIN MODEL ------
model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)

"""There is no implementation of history loss in gensim's Doc2Vec. Sadly this is only a feature that Word2Vec has. So we don't know if 100 epochs was leading to overfitting, it probably did.

Let's save the model. 
"""

model.save("emotionsdoc2vec.model")

"""Let's see if we can separate the documents using only the embeddings distance. """

# Extract the embeddings ---
dv = [model.dv[i] for i in range(len(model.dv))]

# Logical indexing vectors ---
neutral = (train_cats=="neutral").values
neg = (train_cats=="worry").values
pos = (train_cats=="happiness").values

# We need to subset the list with boolean values
from itertools import compress

# Subset document embeddings ---
neutral= list(compress(dv, neutral))
neg= list(compress(dv, neg))
pos= list(compress(dv, pos))

"""Now that we have the embeddings we can calculate the similarity scores. 
In sklearn, if we only provide X, the cosine similarity is calculated between all elements of X, returning a matrix of shape (len(X), len(X)).
"""

from sklearn.metrics.pairwise import cosine_similarity

# Calculate cosine similarity ---
neutral_scores=cosine_similarity(neutral)
neg_scores=cosine_similarity(neg)
pos_scores=cosine_similarity(pos)
# Calculate Mean scores  ---
neutral_scores_mean=neutral_scores.mean()
neg_scores_mean=neg_scores.mean()
pos_scores_mean=pos_scores.mean()
# Print --- 
print("Neutral with itself:", neutral_scores_mean)
print("Negative with itself:", neg_scores_mean)
print("Positive with itself:", pos_scores_mean)

"""We obtained low mean cosine similarities between the elements of each category. That is, neutral documents are close to other neutral documents in average, the same goes to the other 2 categories. 

Also, the average intra-class distance is similar between classes. Although, on average, neutral documents are farther away with other neutral documents than positive ones with other positive documents. 

We can calculate now the similarities between classes: Neutral to both neg and pos and also the similarity between pos and neg.
"""

# Calculate cosine similarity ---
neutral_neg_scores=cosine_similarity(neutral, neg)
neutral_pos_scores=cosine_similarity(neutral, pos)
neg_pos_scores=cosine_similarity(neg, pos)
# Print shapes 
print(neutral_neg_scores.shape)
print(neutral_pos_scores.shape)
print(neg_pos_scores.shape)
# Calculate Mean scores  ---
neutral_neg_scores_mean=neutral_neg_scores.mean()
neutral_pos_scores_mean=neutral_pos_scores.mean()
neg_pos_scores_mean=neg_pos_scores.mean()
# Print --- 
print("Neutral with Negative:", neutral_neg_scores_mean)
print("Neutral with Positive:", neutral_pos_scores_mean)
print("Negative with Positive:", neg_pos_scores_mean)

"""Sadly, there is some noise in the averages. We do not get very good results only comparing cosine distances averages between classes. 

Let's fit a classifier. 

I'll use a multi-variable logistic regression trained on the training set embeddings and classes. The logistic regression will then be used with the testing set in two parts: first, infer the document embeddings, second, infer the class.
"""

from sklearn.linear_model import LogisticRegression

log_reg= LogisticRegression(class_weight="balanced")
log_reg.fit(dv,train_cats)
log_reg.score(dv, train_cats) # accuracy

# Predict on testing data ------
  # Infer embedding ---
dv_test= [model.infer_vector(element, steps=100) for element in test_data]
  # Infer class --- 
preds=log_reg.predict(dv_test)

from sklearn.metrics import classification_report
print(classification_report(test_cats, preds))

"""If we try some dimensionality reduction maybe it'll help our classifier.  """

from sklearn.decomposition import PCA
pca = PCA(n_components=4)
data = pca.fit_transform(dv)

log_reg= LogisticRegression(class_weight="balanced")
log_reg.fit(data,train_cats)
log_reg.score(data, train_cats) # accuracy

component_data= pd.DataFrame(data, columns= ["comp1","comp2","comp3","comp4"])
component_data["true_label"]= train_cats.values

import seaborn as sns
sns.scatterplot(data=component_data, x='comp1', y='comp2', hue='true_label')

sns.scatterplot(data=component_data, x='comp1', y='comp3', hue='true_label')

"""Okay in this case our approach failed. Nonetheless, it is normal, not every attempt can work. Still it is good practice to try gensim's doc2vec. 

"""

