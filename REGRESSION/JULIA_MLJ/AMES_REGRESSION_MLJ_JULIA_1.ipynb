{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMES REGRESSION MLJ JULIA 1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilodlt/rtidy-python/blob/main/REGRESSION/JULIA_MLJ/AMES_REGRESSION_MLJ_JULIA_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# <img src=\"https://github.com/JuliaLang/julia-logo-graphics/raw/master/images/julia-logo-color.png\" height=\"100\" /> _Colab Notebook Template_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIeFXS0F0zww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266e7ee1-9178-40fc-9a3f-979851984237"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.6.0\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools Plots\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -n \"$COLAB_GPU\" ] && [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  if [ \"$COLAB_GPU\" = \"1\" ]; then\n",
        "      JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo ''\n",
        "  echo \"Success! Please reload this page and jump to the next section.\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing Julia 1.6.0 on the current Colab Runtime...\n",
            "2021-08-07 08:19:48 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.6/julia-1.6.0-linux-x86_64.tar.gz [112838927/112838927] -> \"/tmp/julia.tar.gz\" [1]\n",
            "Installing Julia package IJulia...\n",
            "Installing Julia package BenchmarkTools...\n",
            "Installing Julia package Plots...\n",
            "Installing IJulia kernel...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.6\n",
            "\n",
            "Success! Please reload this page and jump to the next section.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Checking the Installation\n",
        "The `versioninfo()` function should print your Julia version and some other info about the system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEzvvzCl1i0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede87bad-d508-4f9d-815d-583ebc80d931"
      },
      "source": [
        "versioninfo()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Julia Version 1.6.0\n",
            "Commit f9720dc2eb (2021-03-24 12:55 UTC)\n",
            "Platform Info:\n",
            "  OS: Linux (x86_64-pc-linux-gnu)\n",
            "  CPU: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "  WORD_SIZE: 64\n",
            "  LIBM: libopenlibm\n",
            "  LLVM: libLLVM-11.0.1 (ORCJIT, haswell)\n",
            "Environment:\n",
            "  JULIA_NUM_THREADS = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQlpeR9wNOi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6196d4c1-5f0c-4b06-b869-ee85c61170fb"
      },
      "source": [
        "using BenchmarkTools\n",
        "\n",
        "M = rand(2048, 2048)\n",
        "@benchmark M^2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BenchmarkTools.Trial: 9 samples with 1 evaluation.\n",
              " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m527.366 ms\u001b[22m\u001b[39m … \u001b[35m712.117 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.22% … 23.65%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m557.912 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m569.311 ms\u001b[22m\u001b[39m ± \u001b[32m 56.079 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.99% ±  7.81%\n",
              "\n",
              "  \u001b[39m█\u001b[39m \u001b[39m█\u001b[39m \u001b[39m█\u001b[34m█\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \n",
              "  \u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
              "  527 ms\u001b[90m           Histogram: frequency by time\u001b[39m          712 ms \u001b[0m\u001b[1m<\u001b[22m\n",
              "\n",
              " Memory estimate\u001b[90m: \u001b[39m\u001b[33m32.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m2\u001b[39m."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XciCcMAJOT3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3548b378-5393-45fb-e91c-e409bf14cbb2"
      },
      "source": [
        "if ENV[\"COLAB_GPU\"] == \"1\"\n",
        "    using CUDA\n",
        "\n",
        "    M_gpu = cu(M)\n",
        "    @benchmark CUDA.@sync M_gpu^2\n",
        "else\n",
        "    println(\"No GPU found.\")\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0bcOPbMfe8d"
      },
      "source": [
        "# Following MLJ \n",
        "\n",
        "AMES Tutorial from https://alan-turing-institute.github.io/DataScienceTutorials.jl/ \n",
        "\n",
        "This notebook has 2 series:\n",
        "- Introduction to stacking and training models\n",
        "- Introduction to parameter tuning\n",
        "\n",
        "In this notebook we explore the facilitated models' stacking with MLJ (Ridge and KNN). \n",
        "\n",
        "We fitted both the stacked and the non-stacked models. Altough we found that the ridge regression performed better when used alone. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tavcI3jgPuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e683f4-74cb-4c13-b922-8011061cf1ff"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"MLJ\")\n",
        "Pkg.add(\"PrettyPrinting\")\n",
        "Pkg.add(\"DataFrames\")\n",
        "Pkg.add(\"StatsPlots\")\n",
        "Pkg.add(\"NearestNeighborModels\")\n",
        "Pkg.add(\"MLJMultivariateStatsInterface\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NearestNeighborModels ─ v0.1.6\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.6/Project.toml`\n",
            " \u001b[90m [636a865e] \u001b[39m\u001b[92m+ NearestNeighborModels v0.1.6\u001b[39m\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.6/Manifest.toml`\n",
            " \u001b[90m [636a865e] \u001b[39m\u001b[92m+ NearestNeighborModels v0.1.6\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[32m  ✓ \u001b[39mNearestNeighborModels\n",
            "1 dependency successfully precompiled in 3 seconds (201 already precompiled)\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ038GAygFo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1b7070-cab3-45e2-ce5b-5383bb044048"
      },
      "source": [
        "# LIBRARIES USED ---\n",
        "using MLJ\n",
        "using  PrettyPrinting\n",
        "using StatsPlots\n",
        "import DataFrames: DataFrame\n",
        "import Statistics\n",
        "\n",
        "# Glance at data ---\n",
        "X, y = @load_reduced_ames # We get the ames data like this \n",
        "X = DataFrame(X) \n",
        "@show size(X)\n",
        "first(X, 3) |> pretty # pass first 3 to pretty function"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size(X) = (1456, 12)\n",
            "┌─────────────────────────────────┬────────────┬──────────────────────────────────┬────────────┬─────────────┬────────────┬────────────┬────────────┬──────────────────────────────────┬────────────┬──────────────┬───────────┐\n",
            "│\u001b[1m OverallQual                     \u001b[0m│\u001b[1m GrLivArea  \u001b[0m│\u001b[1m Neighborhood                     \u001b[0m│\u001b[1m x1stFlrSF  \u001b[0m│\u001b[1m TotalBsmtSF \u001b[0m│\u001b[1m BsmtFinSF1 \u001b[0m│\u001b[1m LotArea    \u001b[0m│\u001b[1m GarageCars \u001b[0m│\u001b[1m MSSubClass                       \u001b[0m│\u001b[1m GarageArea \u001b[0m│\u001b[1m YearRemodAdd \u001b[0m│\u001b[1m YearBuilt \u001b[0m│\n",
            "│\u001b[90m CategoricalValue{Int64, UInt32} \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m CategoricalValue{String, UInt32} \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Int64      \u001b[0m│\u001b[90m CategoricalValue{String, UInt32} \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Int64        \u001b[0m│\u001b[90m Int64     \u001b[0m│\n",
            "│\u001b[90m OrderedFactor{10}               \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Multiclass{25}                   \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Count      \u001b[0m│\u001b[90m Multiclass{15}                   \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Count        \u001b[0m│\u001b[90m Count     \u001b[0m│\n",
            "├─────────────────────────────────┼────────────┼──────────────────────────────────┼────────────┼─────────────┼────────────┼────────────┼────────────┼──────────────────────────────────┼────────────┼──────────────┼───────────┤\n",
            "│ 5                               │ 816.0      │ Mitchel                          │ 816.0      │ 816.0       │ 816.0      │ 6600.0     │ 2          │ _20                              │ 816.0      │ 2003         │ 1982      │\n",
            "│ 8                               │ 2028.0     │ Timber                           │ 2028.0     │ 1868.0      │ 1460.0     │ 11443.0    │ 3          │ _20                              │ 880.0      │ 2006         │ 2005      │\n",
            "│ 7                               │ 1509.0     │ Gilbert                          │ 807.0      │ 783.0       │ 0.0        │ 7875.0     │ 2          │ _60                              │ 393.0      │ 2003         │ 2003      │\n",
            "└─────────────────────────────────┴────────────┴──────────────────────────────────┴────────────┴─────────────┴────────────┴────────────┴────────────┴──────────────────────────────────┴────────────┴──────────────┴───────────┘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrI1_PljhPa9"
      },
      "source": [
        "Okay. So we have a total of 12 columns and 1456 rows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K9hNov_hw-V"
      },
      "source": [
        "Target is a continuous vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inozt90ugOWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173c4ebc-6731-47d9-ff3b-5eb496dbe7be"
      },
      "source": [
        "typeof(y) # target is a Float64 vector "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vector{Float64} (alias for Array{Float64, 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFyBfS3aCmdL"
      },
      "source": [
        "But we should get its scientific type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYIYPjG9hkY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1950f5a9-23ab-49e2-f9f5-e9ffd21d92b6"
      },
      "source": [
        "@show y[1:3]\n",
        "scitype(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y[1:3] = [138000.0, 369900.0, 180000.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AbstractVector{Continuous} (alias for AbstractArray{Continuous, 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsx676iJinmQ"
      },
      "source": [
        "The tutorial uses an algorithm names constant regressor. From the documentation of MLJ, we see that we can get info about their models running info().\n",
        "\n",
        "I believe Constant regressor is the same as the dummy regressor of scikit. The idea is to always return the mean for every input. Obviously we would never use a model like this for serious decisions, but it can be a type of **baseline**.\n",
        "\n",
        "If the data is really imbalanced, a dummy regressor can achieve a high accuracy since it will predict, all the time, the class that is overrepresented. A good model would have to obtain better results than this dummy classifier.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ2aRSKXht3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a563d6a-876d-45a0-f8a1-bdab8690929e"
      },
      "source": [
        "info(ConstantRegressor)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[35mConstant regressor (Probabilistic).\u001b[39m\n",
              "\u001b[35m→ based on [MLJModels](https://github.com/alan-turing-institute/MLJModels.jl).\u001b[39m\n",
              "\u001b[35m→ do `@load ConstantRegressor pkg=\"MLJModels\"` to use the model.\u001b[39m\n",
              "\u001b[35m→ do `?ConstantRegressor` for documentation.\u001b[39m\n",
              "(name = \"ConstantRegressor\",\n",
              " package_name = \"MLJModels\",\n",
              " is_supervised = true,\n",
              " deep_properties = (),\n",
              " docstring = \"Constant regressor (Probabilistic).\\n→ based on [MLJModels](https://github.com/alan-turing-institute/MLJModels.jl).\\n→ do `@load ConstantRegressor pkg=\\\"MLJModels\\\"` to use the model.\\n→ do `?ConstantRegressor` for documentation.\",\n",
              " hyperparameter_ranges = (nothing,),\n",
              " hyperparameter_types = (\"Type{D} where D<:Distributions.Sampleable\",),\n",
              " hyperparameters = (:distribution_type,),\n",
              " implemented_methods = [:fitted_params, :predict],\n",
              " is_pure_julia = true,\n",
              " is_wrapper = false,\n",
              " iteration_parameter = nothing,\n",
              " load_path = \"MLJModels.ConstantRegressor\",\n",
              " package_license = \"MIT\",\n",
              " package_url = \"https://github.com/alan-turing-institute/MLJModels.jl\",\n",
              " package_uuid = \"d491faf4-2d78-11e9-2867-c94bc002c0b7\",\n",
              " prediction_type = :probabilistic,\n",
              " supports_class_weights = false,\n",
              " supports_online = false,\n",
              " supports_training_losses = false,\n",
              " supports_weights = false,\n",
              " input_scitype = Table,\n",
              " target_scitype = AbstractVector{Continuous},\n",
              " output_scitype = Unknown,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI9En5hIj8Lj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "862930d6-f9bb-4a9d-dd93-81a74eff18a5"
      },
      "source": [
        "?ConstantRegressor"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{verbatim}\nConstantRegressor(; distribution_type=Distributions.Normal)\n\\end{verbatim}\nA regressor that, for any new input pattern, predicts the univariate probability distribution best fitting the training target data. Use \\texttt{predict\\_mean} to predict the mean value instead.\n\n",
            "text/markdown": "```\nConstantRegressor(; distribution_type=Distributions.Normal)\n```\n\nA regressor that, for any new input pattern, predicts the univariate probability distribution best fitting the training target data. Use `predict_mean` to predict the mean value instead.\n",
            "text/plain": [
              "\u001b[36m  ConstantRegressor(; distribution_type=Distributions.Normal)\u001b[39m\n",
              "\n",
              "  A regressor that, for any new input pattern, predicts the univariate\n",
              "  probability distribution best fitting the training target data. Use\n",
              "  \u001b[36mpredict_mean\u001b[39m to predict the mean value instead."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwrIjfxekFOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d6eadf-b592-493e-8a7f-e36c9e92c6b4"
      },
      "source": [
        "# We can init the model \n",
        "creg = ConstantRegressor() \n",
        "# And wrap it in a machine to passs args\n",
        "cmach = machine(creg, X, y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mMachine{ConstantRegressor,…} @736\u001b[39m trained 0 times; caches data\n",
              "  args: \n",
              "    1:\t\u001b[34mSource @714\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Count}, AbstractVector{Multiclass{25}}, AbstractVector{Multiclass{15}}, AbstractVector{OrderedFactor{10}}}}`\n",
              "    2:\t\u001b[34mSource @596\u001b[39m ⏎ `AbstractVector{Continuous}`\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHaKi8MQkwb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7df9ab-fe08-4db6-c56c-4ba30e8a8d75"
      },
      "source": [
        "# We can know make train and test splits \n",
        "train, test = partition(collect(eachindex(y)), 0.70, shuffle=true); # 70:30 split. collect(eachindex(y)) gives an index vector 1:len(y)\n",
        "fit!(cmach, rows=train) # modify \"inplace\" hence the !\n",
        "ŷ = predict(cmach, rows=test) # a nice y hat \n",
        "ŷ[1:3] |> pprint"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Training \u001b[34mMachine{ConstantRegressor,…} @736\u001b[39m.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:354\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Distributions.Normal{Float64}(μ=179752.7065750736, σ=77612.15226443602),\n",
            " Distributions.Normal{Float64}(μ=179752.7065750736, σ=77612.15226443602),\n",
            " Distributions.Normal{Float64}(μ=179752.7065750736, σ=77612.15226443602)]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhtPOCn6ofQT"
      },
      "source": [
        "As we can see, every distribution is the same, a normal distribution with mean 177590 and sd 74399. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yby_67NHmJha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e553ab-d42d-45b9-adcb-c660deacac9c"
      },
      "source": [
        "ŷ[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Distributions.Normal{Float64}(μ=179752.7065750736, σ=77612.15226443602)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFbAnuiGC-Cn"
      },
      "source": [
        "We can know plot the distribution "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO-tj48Bot7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "a10c7b7e-b723-41df-b815-432d592c8931"
      },
      "source": [
        "gr(size=(300,300))\n",
        "plot(ŷ[1], fill=(0, .5,:orange))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300\" height=\"300\" viewBox=\"0 0 1200 1200\">\n<defs>\n  <clipPath id=\"clip010\">\n    <rect x=\"0\" y=\"0\" width=\"1200\" height=\"1200\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip010)\" d=\"\nM0 1200 L1200 1200 L1200 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip011\">\n    <rect x=\"240\" y=\"120\" width=\"841\" height=\"841\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip010)\" d=\"\nM295.241 1054.74 L1152.76 1054.74 L1152.76 47.2441 L295.241 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip012\">\n    <rect x=\"295\" y=\"47\" width=\"859\" height=\"1008\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  331.966,1054.74 331.966,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  472.101,1054.74 472.101,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  612.237,1054.74 612.237,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  752.372,1054.74 752.372,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  892.508,1054.74 892.508,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1032.64,1054.74 1032.64,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,1054.74 1152.76,1054.74 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  331.966,1054.74 331.966,1035.85 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  472.101,1054.74 472.101,1035.85 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  612.237,1054.74 612.237,1035.85 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  752.372,1054.74 752.372,1035.85 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  892.508,1054.74 892.508,1035.85 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1032.64,1054.74 1032.64,1035.85 \n  \"/>\n<path clip-path=\"url(#clip010)\" d=\"M219.387 1110.52 L249.063 1110.52 L249.063 1114.46 L219.387 1114.46 L219.387 1110.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M259.966 1123.42 L267.604 1123.42 L267.604 1097.05 L259.294 1098.72 L259.294 1094.46 L267.558 1092.79 L272.234 1092.79 L272.234 1123.42 L279.873 1123.42 L279.873 1127.35 L259.966 1127.35 L259.966 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M289.317 1121.47 L294.201 1121.47 L294.201 1127.35 L289.317 1127.35 L289.317 1121.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M314.387 1095.87 Q310.775 1095.87 308.947 1099.43 Q307.141 1102.98 307.141 1110.1 Q307.141 1117.21 308.947 1120.78 Q310.775 1124.32 314.387 1124.32 Q318.021 1124.32 319.826 1120.78 Q321.655 1117.21 321.655 1110.1 Q321.655 1102.98 319.826 1099.43 Q318.021 1095.87 314.387 1095.87 M314.387 1092.17 Q320.197 1092.17 323.252 1096.77 Q326.331 1101.36 326.331 1110.1 Q326.331 1118.83 323.252 1123.44 Q320.197 1128.02 314.387 1128.02 Q308.576 1128.02 305.498 1123.44 Q302.442 1118.83 302.442 1110.1 Q302.442 1101.36 305.498 1096.77 Q308.576 1092.17 314.387 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M362.72 1101.89 L352.141 1112.51 L362.72 1123.09 L359.965 1125.89 L349.34 1115.27 L338.715 1125.89 L335.984 1123.09 L346.539 1112.51 L335.984 1101.89 L338.715 1099.09 L349.34 1109.71 L359.965 1099.09 L362.72 1101.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M375.081 1123.42 L382.719 1123.42 L382.719 1097.05 L374.409 1098.72 L374.409 1094.46 L382.673 1092.79 L387.349 1092.79 L387.349 1123.42 L394.988 1123.42 L394.988 1127.35 L375.081 1127.35 L375.081 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M414.432 1095.87 Q410.821 1095.87 408.992 1099.43 Q407.187 1102.98 407.187 1110.1 Q407.187 1117.21 408.992 1120.78 Q410.821 1124.32 414.432 1124.32 Q418.067 1124.32 419.872 1120.78 Q421.701 1117.21 421.701 1110.1 Q421.701 1102.98 419.872 1099.43 Q418.067 1095.87 414.432 1095.87 M414.432 1092.17 Q420.242 1092.17 423.298 1096.77 Q426.377 1101.36 426.377 1110.1 Q426.377 1118.83 423.298 1123.44 Q420.242 1128.02 414.432 1128.02 Q408.622 1128.02 405.543 1123.44 Q402.488 1118.83 402.488 1110.1 Q402.488 1101.36 405.543 1096.77 Q408.622 1092.17 414.432 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M427.562 1071.86 L442.476 1071.86 L442.476 1075.06 L431.041 1075.06 L431.041 1081.94 Q431.869 1081.66 432.696 1081.53 Q433.524 1081.38 434.351 1081.38 Q439.053 1081.38 441.799 1083.95 Q444.545 1086.53 444.545 1090.93 Q444.545 1095.46 441.724 1097.98 Q438.903 1100.49 433.768 1100.49 Q432 1100.49 430.157 1100.18 Q428.333 1099.88 426.377 1099.28 L426.377 1095.46 Q428.069 1096.39 429.875 1096.84 Q431.68 1097.29 433.693 1097.29 Q436.947 1097.29 438.846 1095.58 Q440.746 1093.86 440.746 1090.93 Q440.746 1088 438.846 1086.29 Q436.947 1084.57 433.693 1084.57 Q432.169 1084.57 430.646 1084.91 Q429.141 1085.25 427.562 1085.97 L427.562 1071.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M472.101 1074.33 Q468.49 1074.33 466.662 1077.9 Q464.856 1081.44 464.856 1088.57 Q464.856 1095.68 466.662 1099.24 Q468.49 1102.78 472.101 1102.78 Q475.736 1102.78 477.541 1099.24 Q479.37 1095.68 479.37 1088.57 Q479.37 1081.44 477.541 1077.9 Q475.736 1074.33 472.101 1074.33 M472.101 1070.63 Q477.912 1070.63 480.967 1075.24 Q484.046 1079.82 484.046 1088.57 Q484.046 1097.3 480.967 1101.9 Q477.912 1106.49 472.101 1106.49 Q466.291 1106.49 463.213 1101.9 Q460.157 1097.3 460.157 1088.57 Q460.157 1079.82 463.213 1075.24 Q466.291 1070.63 472.101 1070.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M520.283 1123.42 L527.922 1123.42 L527.922 1097.05 L519.611 1098.72 L519.611 1094.46 L527.875 1092.79 L532.551 1092.79 L532.551 1123.42 L540.19 1123.42 L540.19 1127.35 L520.283 1127.35 L520.283 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M549.634 1121.47 L554.519 1121.47 L554.519 1127.35 L549.634 1127.35 L549.634 1121.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M574.704 1095.87 Q571.093 1095.87 569.264 1099.43 Q567.458 1102.98 567.458 1110.1 Q567.458 1117.21 569.264 1120.78 Q571.093 1124.32 574.704 1124.32 Q578.338 1124.32 580.144 1120.78 Q581.972 1117.21 581.972 1110.1 Q581.972 1102.98 580.144 1099.43 Q578.338 1095.87 574.704 1095.87 M574.704 1092.17 Q580.514 1092.17 583.569 1096.77 Q586.648 1101.36 586.648 1110.1 Q586.648 1118.83 583.569 1123.44 Q580.514 1128.02 574.704 1128.02 Q568.894 1128.02 565.815 1123.44 Q562.759 1118.83 562.759 1110.1 Q562.759 1101.36 565.815 1096.77 Q568.894 1092.17 574.704 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M623.037 1101.89 L612.458 1112.51 L623.037 1123.09 L620.282 1125.89 L609.657 1115.27 L599.032 1125.89 L596.301 1123.09 L606.856 1112.51 L596.301 1101.89 L599.032 1099.09 L609.657 1109.71 L620.282 1099.09 L623.037 1101.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M635.398 1123.42 L643.037 1123.42 L643.037 1097.05 L634.727 1098.72 L634.727 1094.46 L642.99 1092.79 L647.666 1092.79 L647.666 1123.42 L655.305 1123.42 L655.305 1127.35 L635.398 1127.35 L635.398 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M674.749 1095.87 Q671.138 1095.87 669.31 1099.43 Q667.504 1102.98 667.504 1110.1 Q667.504 1117.21 669.31 1120.78 Q671.138 1124.32 674.749 1124.32 Q678.384 1124.32 680.189 1120.78 Q682.018 1117.21 682.018 1110.1 Q682.018 1102.98 680.189 1099.43 Q678.384 1095.87 674.749 1095.87 M674.749 1092.17 Q680.56 1092.17 683.615 1096.77 Q686.694 1101.36 686.694 1110.1 Q686.694 1118.83 683.615 1123.44 Q680.56 1128.02 674.749 1128.02 Q668.939 1128.02 665.861 1123.44 Q662.805 1118.83 662.805 1110.1 Q662.805 1101.36 665.861 1096.77 Q668.939 1092.17 674.749 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M687.879 1071.86 L702.793 1071.86 L702.793 1075.06 L691.358 1075.06 L691.358 1081.94 Q692.186 1081.66 693.013 1081.53 Q693.841 1081.38 694.668 1081.38 Q699.37 1081.38 702.116 1083.95 Q704.862 1086.53 704.862 1090.93 Q704.862 1095.46 702.041 1097.98 Q699.22 1100.49 694.085 1100.49 Q692.317 1100.49 690.474 1100.18 Q688.65 1099.88 686.694 1099.28 L686.694 1095.46 Q688.387 1096.39 690.192 1096.84 Q691.998 1097.29 694.01 1097.29 Q697.264 1097.29 699.163 1095.58 Q701.063 1093.86 701.063 1090.93 Q701.063 1088 699.163 1086.29 Q697.264 1084.57 694.01 1084.57 Q692.487 1084.57 690.963 1084.91 Q689.459 1085.25 687.879 1085.97 L687.879 1071.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M664.504 1123.42 L680.823 1123.42 L680.823 1127.35 L658.879 1127.35 L658.879 1123.42 Q661.541 1120.66 666.124 1116.03 Q670.731 1111.38 671.911 1110.04 Q674.156 1107.51 675.036 1105.78 Q675.939 1104.02 675.939 1102.33 Q675.939 1099.57 673.994 1097.84 Q672.073 1096.1 668.971 1096.1 Q666.772 1096.1 664.319 1096.86 Q661.888 1097.63 659.11 1099.18 L659.11 1094.46 Q661.934 1093.32 664.388 1092.74 Q666.842 1092.17 668.879 1092.17 Q674.249 1092.17 677.443 1094.85 Q680.638 1097.54 680.638 1102.03 Q680.638 1104.16 679.828 1106.08 Q679.041 1107.98 676.934 1110.57 Q676.356 1111.24 673.254 1114.46 Q670.152 1117.65 664.504 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M690.638 1121.47 L695.522 1121.47 L695.522 1127.35 L690.638 1127.35 L690.638 1121.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M715.707 1095.87 Q712.096 1095.87 710.267 1099.43 Q708.462 1102.98 708.462 1110.1 Q708.462 1117.21 710.267 1120.78 Q712.096 1124.32 715.707 1124.32 Q719.341 1124.32 721.147 1120.78 Q722.976 1117.21 722.976 1110.1 Q722.976 1102.98 721.147 1099.43 Q719.341 1095.87 715.707 1095.87 M715.707 1092.17 Q721.517 1092.17 724.573 1096.77 Q727.652 1101.36 727.652 1110.1 Q727.652 1118.83 724.573 1123.44 Q721.517 1128.02 715.707 1128.02 Q709.897 1128.02 706.818 1123.44 Q703.763 1118.83 703.763 1110.1 Q703.763 1101.36 706.818 1096.77 Q709.897 1092.17 715.707 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M764.04 1101.89 L753.462 1112.51 L764.04 1123.09 L761.286 1125.89 L750.661 1115.27 L740.036 1125.89 L737.304 1123.09 L747.86 1112.51 L737.304 1101.89 L740.036 1099.09 L750.661 1109.71 L761.286 1099.09 L764.04 1101.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M776.401 1123.42 L784.04 1123.42 L784.04 1097.05 L775.73 1098.72 L775.73 1094.46 L783.994 1092.79 L788.67 1092.79 L788.67 1123.42 L796.309 1123.42 L796.309 1127.35 L776.401 1127.35 L776.401 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M815.753 1095.87 Q812.142 1095.87 810.313 1099.43 Q808.508 1102.98 808.508 1110.1 Q808.508 1117.21 810.313 1120.78 Q812.142 1124.32 815.753 1124.32 Q819.387 1124.32 821.193 1120.78 Q823.021 1117.21 823.021 1110.1 Q823.021 1102.98 821.193 1099.43 Q819.387 1095.87 815.753 1095.87 M815.753 1092.17 Q821.563 1092.17 824.619 1096.77 Q827.697 1101.36 827.697 1110.1 Q827.697 1118.83 824.619 1123.44 Q821.563 1128.02 815.753 1128.02 Q809.943 1128.02 806.864 1123.44 Q803.809 1118.83 803.809 1110.1 Q803.809 1101.36 806.864 1096.77 Q809.943 1092.17 815.753 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M828.882 1071.86 L843.797 1071.86 L843.797 1075.06 L832.362 1075.06 L832.362 1081.94 Q833.189 1081.66 834.017 1081.53 Q834.844 1081.38 835.672 1081.38 Q840.374 1081.38 843.12 1083.95 Q845.866 1086.53 845.866 1090.93 Q845.866 1095.46 843.044 1097.98 Q840.223 1100.49 835.089 1100.49 Q833.321 1100.49 831.478 1100.18 Q829.653 1099.88 827.697 1099.28 L827.697 1095.46 Q829.39 1096.39 831.196 1096.84 Q833.001 1097.29 835.014 1097.29 Q838.267 1097.29 840.167 1095.58 Q842.066 1093.86 842.066 1090.93 Q842.066 1088 840.167 1086.29 Q838.267 1084.57 835.014 1084.57 Q833.49 1084.57 831.967 1084.91 Q830.462 1085.25 828.882 1085.97 L828.882 1071.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M814.709 1108.72 Q818.065 1109.43 819.94 1111.7 Q821.838 1113.97 821.838 1117.3 Q821.838 1122.42 818.32 1125.22 Q814.801 1128.02 808.32 1128.02 Q806.144 1128.02 803.829 1127.58 Q801.537 1127.17 799.084 1126.31 L799.084 1121.79 Q801.028 1122.93 803.343 1123.51 Q805.658 1124.09 808.181 1124.09 Q812.579 1124.09 814.871 1122.35 Q817.185 1120.61 817.185 1117.3 Q817.185 1114.25 815.033 1112.54 Q812.903 1110.8 809.084 1110.8 L805.056 1110.8 L805.056 1106.96 L809.269 1106.96 Q812.718 1106.96 814.547 1105.59 Q816.375 1104.2 816.375 1101.61 Q816.375 1098.95 814.477 1097.54 Q812.602 1096.1 809.084 1096.1 Q807.162 1096.1 804.963 1096.52 Q802.764 1096.93 800.125 1097.81 L800.125 1093.65 Q802.787 1092.91 805.102 1092.54 Q807.44 1092.17 809.5 1092.17 Q814.824 1092.17 817.926 1094.6 Q821.028 1097 821.028 1101.12 Q821.028 1103.99 819.384 1105.98 Q817.741 1107.95 814.709 1108.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M830.704 1121.47 L835.588 1121.47 L835.588 1127.35 L830.704 1127.35 L830.704 1121.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M855.773 1095.87 Q852.162 1095.87 850.333 1099.43 Q848.528 1102.98 848.528 1110.1 Q848.528 1117.21 850.333 1120.78 Q852.162 1124.32 855.773 1124.32 Q859.407 1124.32 861.213 1120.78 Q863.042 1117.21 863.042 1110.1 Q863.042 1102.98 861.213 1099.43 Q859.407 1095.87 855.773 1095.87 M855.773 1092.17 Q861.583 1092.17 864.639 1096.77 Q867.718 1101.36 867.718 1110.1 Q867.718 1118.83 864.639 1123.44 Q861.583 1128.02 855.773 1128.02 Q849.963 1128.02 846.884 1123.44 Q843.829 1118.83 843.829 1110.1 Q843.829 1101.36 846.884 1096.77 Q849.963 1092.17 855.773 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M904.106 1101.89 L893.528 1112.51 L904.106 1123.09 L901.352 1125.89 L890.727 1115.27 L880.102 1125.89 L877.37 1123.09 L887.926 1112.51 L877.37 1101.89 L880.102 1099.09 L890.727 1109.71 L901.352 1099.09 L904.106 1101.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M916.467 1123.42 L924.106 1123.42 L924.106 1097.05 L915.796 1098.72 L915.796 1094.46 L924.06 1092.79 L928.736 1092.79 L928.736 1123.42 L936.375 1123.42 L936.375 1127.35 L916.467 1127.35 L916.467 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M955.819 1095.87 Q952.208 1095.87 950.379 1099.43 Q948.574 1102.98 948.574 1110.1 Q948.574 1117.21 950.379 1120.78 Q952.208 1124.32 955.819 1124.32 Q959.453 1124.32 961.259 1120.78 Q963.087 1117.21 963.087 1110.1 Q963.087 1102.98 961.259 1099.43 Q959.453 1095.87 955.819 1095.87 M955.819 1092.17 Q961.629 1092.17 964.685 1096.77 Q967.763 1101.36 967.763 1110.1 Q967.763 1118.83 964.685 1123.44 Q961.629 1128.02 955.819 1128.02 Q950.009 1128.02 946.93 1123.44 Q943.875 1118.83 943.875 1110.1 Q943.875 1101.36 946.93 1096.77 Q950.009 1092.17 955.819 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M968.948 1071.86 L983.863 1071.86 L983.863 1075.06 L972.428 1075.06 L972.428 1081.94 Q973.255 1081.66 974.083 1081.53 Q974.91 1081.38 975.738 1081.38 Q980.44 1081.38 983.186 1083.95 Q985.932 1086.53 985.932 1090.93 Q985.932 1095.46 983.11 1097.98 Q980.289 1100.49 975.155 1100.49 Q973.387 1100.49 971.544 1100.18 Q969.719 1099.88 967.763 1099.28 L967.763 1095.46 Q969.456 1096.39 971.262 1096.84 Q973.067 1097.29 975.079 1097.29 Q978.333 1097.29 980.233 1095.58 Q982.132 1093.86 982.132 1090.93 Q982.132 1088 980.233 1086.29 Q978.333 1084.57 975.079 1084.57 Q973.556 1084.57 972.033 1084.91 Q970.528 1085.25 968.948 1085.97 L968.948 1071.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M954.173 1096.86 L942.367 1115.31 L954.173 1115.31 L954.173 1096.86 M952.946 1092.79 L958.825 1092.79 L958.825 1115.31 L963.756 1115.31 L963.756 1119.2 L958.825 1119.2 L958.825 1127.35 L954.173 1127.35 L954.173 1119.2 L938.571 1119.2 L938.571 1114.69 L952.946 1092.79 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M971.487 1121.47 L976.372 1121.47 L976.372 1127.35 L971.487 1127.35 L971.487 1121.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M996.557 1095.87 Q992.946 1095.87 991.117 1099.43 Q989.311 1102.98 989.311 1110.1 Q989.311 1117.21 991.117 1120.78 Q992.946 1124.32 996.557 1124.32 Q1000.19 1124.32 1002 1120.78 Q1003.83 1117.21 1003.83 1110.1 Q1003.83 1102.98 1002 1099.43 Q1000.19 1095.87 996.557 1095.87 M996.557 1092.17 Q1002.37 1092.17 1005.42 1096.77 Q1008.5 1101.36 1008.5 1110.1 Q1008.5 1118.83 1005.42 1123.44 Q1002.37 1128.02 996.557 1128.02 Q990.747 1128.02 987.668 1123.44 Q984.612 1118.83 984.612 1110.1 Q984.612 1101.36 987.668 1096.77 Q990.747 1092.17 996.557 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1044.89 1101.89 L1034.31 1112.51 L1044.89 1123.09 L1042.14 1125.89 L1031.51 1115.27 L1020.89 1125.89 L1018.15 1123.09 L1028.71 1112.51 L1018.15 1101.89 L1020.89 1099.09 L1031.51 1109.71 L1042.14 1099.09 L1044.89 1101.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1057.25 1123.42 L1064.89 1123.42 L1064.89 1097.05 L1056.58 1098.72 L1056.58 1094.46 L1064.84 1092.79 L1069.52 1092.79 L1069.52 1123.42 L1077.16 1123.42 L1077.16 1127.35 L1057.25 1127.35 L1057.25 1123.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1096.6 1095.87 Q1092.99 1095.87 1091.16 1099.43 Q1089.36 1102.98 1089.36 1110.1 Q1089.36 1117.21 1091.16 1120.78 Q1092.99 1124.32 1096.6 1124.32 Q1100.24 1124.32 1102.04 1120.78 Q1103.87 1117.21 1103.87 1110.1 Q1103.87 1102.98 1102.04 1099.43 Q1100.24 1095.87 1096.6 1095.87 M1096.6 1092.17 Q1102.41 1092.17 1105.47 1096.77 Q1108.55 1101.36 1108.55 1110.1 Q1108.55 1118.83 1105.47 1123.44 Q1102.41 1128.02 1096.6 1128.02 Q1090.79 1128.02 1087.71 1123.44 Q1084.66 1118.83 1084.66 1110.1 Q1084.66 1101.36 1087.71 1096.77 Q1090.79 1092.17 1096.6 1092.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1109.73 1071.86 L1124.65 1071.86 L1124.65 1075.06 L1113.21 1075.06 L1113.21 1081.94 Q1114.04 1081.66 1114.87 1081.53 Q1115.69 1081.38 1116.52 1081.38 Q1121.22 1081.38 1123.97 1083.95 Q1126.72 1086.53 1126.72 1090.93 Q1126.72 1095.46 1123.89 1097.98 Q1121.07 1100.49 1115.94 1100.49 Q1114.17 1100.49 1112.33 1100.18 Q1110.5 1099.88 1108.55 1099.28 L1108.55 1095.46 Q1110.24 1096.39 1112.05 1096.84 Q1113.85 1097.29 1115.86 1097.29 Q1119.12 1097.29 1121.02 1095.58 Q1122.92 1093.86 1122.92 1090.93 Q1122.92 1088 1121.02 1086.29 Q1119.12 1084.57 1115.86 1084.57 Q1114.34 1084.57 1112.82 1084.91 Q1111.31 1085.25 1109.73 1085.97 L1109.73 1071.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  295.241,1026.23 1152.76,1026.23 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  295.241,841.289 1152.76,841.289 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  295.241,656.348 1152.76,656.348 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  295.241,471.408 1152.76,471.408 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  295.241,286.468 1152.76,286.468 \n  \"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  295.241,101.527 1152.76,101.527 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,1054.74 295.241,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,1026.23 314.139,1026.23 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,841.289 314.139,841.289 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,656.348 314.139,656.348 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,471.408 314.139,471.408 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,286.468 314.139,286.468 \n  \"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  295.241,101.527 314.139,101.527 \n  \"/>\n<path clip-path=\"url(#clip010)\" d=\"M265.297 1012.03 Q261.686 1012.03 259.857 1015.59 Q258.052 1019.13 258.052 1026.26 Q258.052 1033.37 259.857 1036.93 Q261.686 1040.48 265.297 1040.48 Q268.931 1040.48 270.737 1036.93 Q272.565 1033.37 272.565 1026.26 Q272.565 1019.13 270.737 1015.59 Q268.931 1012.03 265.297 1012.03 M265.297 1008.32 Q271.107 1008.32 274.163 1012.93 Q277.241 1017.51 277.241 1026.26 Q277.241 1034.99 274.163 1039.6 Q271.107 1044.18 265.297 1044.18 Q259.487 1044.18 256.408 1039.6 Q253.352 1034.99 253.352 1026.26 Q253.352 1017.51 256.408 1012.93 Q259.487 1008.32 265.297 1008.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M60.5569 861.081 L68.1958 861.081 L68.1958 834.715 L59.8856 836.382 L59.8856 832.123 L68.1495 830.456 L72.8254 830.456 L72.8254 861.081 L80.4642 861.081 L80.4642 865.016 L60.5569 865.016 L60.5569 861.081 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M89.9086 859.137 L94.7928 859.137 L94.7928 865.016 L89.9086 865.016 L89.9086 859.137 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M114.978 833.535 Q111.367 833.535 109.538 837.1 Q107.733 840.641 107.733 847.771 Q107.733 854.877 109.538 858.442 Q111.367 861.984 114.978 861.984 Q118.612 861.984 120.418 858.442 Q122.246 854.877 122.246 847.771 Q122.246 840.641 120.418 837.1 Q118.612 833.535 114.978 833.535 M114.978 829.831 Q120.788 829.831 123.844 834.438 Q126.922 839.021 126.922 847.771 Q126.922 856.498 123.844 861.104 Q120.788 865.688 114.978 865.688 Q109.168 865.688 106.089 861.104 Q103.034 856.498 103.034 847.771 Q103.034 839.021 106.089 834.438 Q109.168 829.831 114.978 829.831 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M163.311 839.553 L152.732 850.178 L163.311 860.757 L160.556 863.558 L149.931 852.933 L139.306 863.558 L136.575 860.757 L147.131 850.178 L136.575 839.553 L139.306 836.753 L149.931 847.377 L160.556 836.753 L163.311 839.553 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M175.672 861.081 L183.311 861.081 L183.311 834.715 L175.001 836.382 L175.001 832.123 L183.265 830.456 L187.94 830.456 L187.94 861.081 L195.579 861.081 L195.579 865.016 L175.672 865.016 L175.672 861.081 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M215.024 833.535 Q211.413 833.535 209.584 837.1 Q207.778 840.641 207.778 847.771 Q207.778 854.877 209.584 858.442 Q211.413 861.984 215.024 861.984 Q218.658 861.984 220.463 858.442 Q222.292 854.877 222.292 847.771 Q222.292 840.641 220.463 837.1 Q218.658 833.535 215.024 833.535 M215.024 829.831 Q220.834 829.831 223.889 834.438 Q226.968 839.021 226.968 847.771 Q226.968 856.498 223.889 861.104 Q220.834 865.688 215.024 865.688 Q209.214 865.688 206.135 861.104 Q203.079 856.498 203.079 847.771 Q203.079 839.021 206.135 834.438 Q209.214 829.831 215.024 829.831 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M226.968 823.933 L251.08 823.933 L251.08 827.13 L226.968 827.13 L226.968 823.933 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M267.875 822.052 Q265.317 822.052 263.812 823.801 Q262.327 825.55 262.327 828.597 Q262.327 831.625 263.812 833.393 Q265.317 835.142 267.875 835.142 Q270.433 835.142 271.919 833.393 Q273.423 831.625 273.423 828.597 Q273.423 825.55 271.919 823.801 Q270.433 822.052 267.875 822.052 M275.417 810.147 L275.417 813.607 Q273.987 812.93 272.52 812.573 Q271.072 812.215 269.643 812.215 Q265.881 812.215 263.888 814.754 Q261.913 817.293 261.631 822.428 Q262.74 820.792 264.414 819.927 Q266.088 819.043 268.101 819.043 Q272.332 819.043 274.777 821.619 Q277.241 824.177 277.241 828.597 Q277.241 832.923 274.683 835.537 Q272.126 838.151 267.875 838.151 Q263.004 838.151 260.427 834.427 Q257.85 830.685 257.85 823.594 Q257.85 816.936 261.01 812.986 Q264.17 809.018 269.492 809.018 Q270.922 809.018 272.37 809.3 Q273.837 809.582 275.417 810.147 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M63.7745 676.141 L80.0939 676.141 L80.0939 680.076 L58.1495 680.076 L58.1495 676.141 Q60.8115 673.386 65.3949 668.757 Q70.0013 664.104 71.1819 662.761 Q73.4272 660.238 74.3068 658.502 Q75.2096 656.743 75.2096 655.053 Q75.2096 652.298 73.2652 650.562 Q71.3439 648.826 68.2421 648.826 Q66.043 648.826 63.5893 649.59 Q61.1588 650.354 58.381 651.905 L58.381 647.183 Q61.2051 646.048 63.6588 645.47 Q66.1124 644.891 68.1495 644.891 Q73.5198 644.891 76.7142 647.576 Q79.9087 650.261 79.9087 654.752 Q79.9087 656.882 79.0985 658.803 Q78.3115 660.701 76.205 663.294 Q75.6263 663.965 72.5245 667.182 Q69.4226 670.377 63.7745 676.141 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M89.9086 674.196 L94.7928 674.196 L94.7928 680.076 L89.9086 680.076 L89.9086 674.196 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M114.978 648.595 Q111.367 648.595 109.538 652.159 Q107.733 655.701 107.733 662.831 Q107.733 669.937 109.538 673.502 Q111.367 677.044 114.978 677.044 Q118.612 677.044 120.418 673.502 Q122.246 669.937 122.246 662.831 Q122.246 655.701 120.418 652.159 Q118.612 648.595 114.978 648.595 M114.978 644.891 Q120.788 644.891 123.844 649.497 Q126.922 654.081 126.922 662.831 Q126.922 671.557 123.844 676.164 Q120.788 680.747 114.978 680.747 Q109.168 680.747 106.089 676.164 Q103.034 671.557 103.034 662.831 Q103.034 654.081 106.089 649.497 Q109.168 644.891 114.978 644.891 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M163.311 654.613 L152.732 665.238 L163.311 675.817 L160.556 678.618 L149.931 667.993 L139.306 678.618 L136.575 675.817 L147.131 665.238 L136.575 654.613 L139.306 651.812 L149.931 662.437 L160.556 651.812 L163.311 654.613 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M175.672 676.141 L183.311 676.141 L183.311 649.775 L175.001 651.442 L175.001 647.183 L183.265 645.516 L187.94 645.516 L187.94 676.141 L195.579 676.141 L195.579 680.076 L175.672 680.076 L175.672 676.141 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M215.024 648.595 Q211.413 648.595 209.584 652.159 Q207.778 655.701 207.778 662.831 Q207.778 669.937 209.584 673.502 Q211.413 677.044 215.024 677.044 Q218.658 677.044 220.463 673.502 Q222.292 669.937 222.292 662.831 Q222.292 655.701 220.463 652.159 Q218.658 648.595 215.024 648.595 M215.024 644.891 Q220.834 644.891 223.889 649.497 Q226.968 654.081 226.968 662.831 Q226.968 671.557 223.889 676.164 Q220.834 680.747 215.024 680.747 Q209.214 680.747 206.135 676.164 Q203.079 671.557 203.079 662.831 Q203.079 654.081 206.135 649.497 Q209.214 644.891 215.024 644.891 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M226.968 638.992 L251.08 638.992 L251.08 642.19 L226.968 642.19 L226.968 638.992 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M267.875 637.112 Q265.317 637.112 263.812 638.861 Q262.327 640.61 262.327 643.657 Q262.327 646.685 263.812 648.453 Q265.317 650.202 267.875 650.202 Q270.433 650.202 271.919 648.453 Q273.423 646.685 273.423 643.657 Q273.423 640.61 271.919 638.861 Q270.433 637.112 267.875 637.112 M275.417 625.206 L275.417 628.667 Q273.987 627.99 272.52 627.632 Q271.072 627.275 269.643 627.275 Q265.881 627.275 263.888 629.814 Q261.913 632.353 261.631 637.488 Q262.74 635.851 264.414 634.986 Q266.088 634.102 268.101 634.102 Q272.332 634.102 274.777 636.679 Q277.241 639.237 277.241 643.657 Q277.241 647.982 274.683 650.597 Q272.126 653.211 267.875 653.211 Q263.004 653.211 260.427 649.487 Q257.85 645.744 257.85 638.654 Q257.85 631.996 261.01 628.046 Q264.17 624.078 269.492 624.078 Q270.922 624.078 272.37 624.36 Q273.837 624.642 275.417 625.206 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M73.9133 476.501 Q77.2698 477.219 79.1448 479.488 Q81.0429 481.756 81.0429 485.089 Q81.0429 490.205 77.5244 493.006 Q74.0059 495.807 67.5245 495.807 Q65.3486 495.807 63.0338 495.367 Q60.7421 494.95 58.2884 494.094 L58.2884 489.58 Q60.2328 490.714 62.5477 491.293 Q64.8625 491.872 67.3856 491.872 Q71.7837 491.872 74.0754 490.136 Q76.3902 488.4 76.3902 485.089 Q76.3902 482.034 74.2374 480.321 Q72.1078 478.585 68.2884 478.585 L64.2606 478.585 L64.2606 474.742 L68.4735 474.742 Q71.9226 474.742 73.7513 473.376 Q75.58 471.988 75.58 469.395 Q75.58 466.733 73.6819 465.321 Q71.8069 463.886 68.2884 463.886 Q66.3671 463.886 64.168 464.302 Q61.969 464.719 59.3301 465.599 L59.3301 461.432 Q61.9921 460.691 64.3069 460.321 Q66.6449 459.951 68.705 459.951 Q74.0291 459.951 77.1309 462.381 Q80.2327 464.789 80.2327 468.909 Q80.2327 471.779 78.5892 473.77 Q76.9457 475.738 73.9133 476.501 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M89.9086 489.256 L94.7928 489.256 L94.7928 495.136 L89.9086 495.136 L89.9086 489.256 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M114.978 463.654 Q111.367 463.654 109.538 467.219 Q107.733 470.761 107.733 477.89 Q107.733 484.997 109.538 488.562 Q111.367 492.103 114.978 492.103 Q118.612 492.103 120.418 488.562 Q122.246 484.997 122.246 477.89 Q122.246 470.761 120.418 467.219 Q118.612 463.654 114.978 463.654 M114.978 459.951 Q120.788 459.951 123.844 464.557 Q126.922 469.14 126.922 477.89 Q126.922 486.617 123.844 491.224 Q120.788 495.807 114.978 495.807 Q109.168 495.807 106.089 491.224 Q103.034 486.617 103.034 477.89 Q103.034 469.14 106.089 464.557 Q109.168 459.951 114.978 459.951 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M163.311 469.673 L152.732 480.298 L163.311 490.876 L160.556 493.677 L149.931 483.052 L139.306 493.677 L136.575 490.876 L147.131 480.298 L136.575 469.673 L139.306 466.872 L149.931 477.497 L160.556 466.872 L163.311 469.673 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M175.672 491.2 L183.311 491.2 L183.311 464.835 L175.001 466.502 L175.001 462.242 L183.265 460.576 L187.94 460.576 L187.94 491.2 L195.579 491.2 L195.579 495.136 L175.672 495.136 L175.672 491.2 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M215.024 463.654 Q211.413 463.654 209.584 467.219 Q207.778 470.761 207.778 477.89 Q207.778 484.997 209.584 488.562 Q211.413 492.103 215.024 492.103 Q218.658 492.103 220.463 488.562 Q222.292 484.997 222.292 477.89 Q222.292 470.761 220.463 467.219 Q218.658 463.654 215.024 463.654 M215.024 459.951 Q220.834 459.951 223.889 464.557 Q226.968 469.14 226.968 477.89 Q226.968 486.617 223.889 491.224 Q220.834 495.807 215.024 495.807 Q209.214 495.807 206.135 491.224 Q203.079 486.617 203.079 477.89 Q203.079 469.14 206.135 464.557 Q209.214 459.951 215.024 459.951 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M226.968 454.052 L251.08 454.052 L251.08 457.249 L226.968 457.249 L226.968 454.052 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M267.875 452.171 Q265.317 452.171 263.812 453.92 Q262.327 455.669 262.327 458.716 Q262.327 461.744 263.812 463.512 Q265.317 465.261 267.875 465.261 Q270.433 465.261 271.919 463.512 Q273.423 461.744 273.423 458.716 Q273.423 455.669 271.919 453.92 Q270.433 452.171 267.875 452.171 M275.417 440.266 L275.417 443.727 Q273.987 443.049 272.52 442.692 Q271.072 442.335 269.643 442.335 Q265.881 442.335 263.888 444.874 Q261.913 447.413 261.631 452.547 Q262.74 450.911 264.414 450.046 Q266.088 449.162 268.101 449.162 Q272.332 449.162 274.777 451.739 Q277.241 454.296 277.241 458.716 Q277.241 463.042 274.683 465.656 Q272.126 468.271 267.875 468.271 Q263.004 468.271 260.427 464.547 Q257.85 460.804 257.85 453.713 Q257.85 447.056 261.01 443.106 Q264.17 439.137 269.492 439.137 Q270.922 439.137 272.37 439.42 Q273.837 439.702 275.417 440.266 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M72.5939 279.709 L60.7884 298.158 L72.5939 298.158 L72.5939 279.709 M71.367 275.635 L77.2466 275.635 L77.2466 298.158 L82.1772 298.158 L82.1772 302.047 L77.2466 302.047 L77.2466 310.195 L72.5939 310.195 L72.5939 302.047 L56.9921 302.047 L56.9921 297.533 L71.367 275.635 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M89.9086 304.316 L94.7928 304.316 L94.7928 310.195 L89.9086 310.195 L89.9086 304.316 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M114.978 278.714 Q111.367 278.714 109.538 282.279 Q107.733 285.82 107.733 292.95 Q107.733 300.057 109.538 303.621 Q111.367 307.163 114.978 307.163 Q118.612 307.163 120.418 303.621 Q122.246 300.057 122.246 292.95 Q122.246 285.82 120.418 282.279 Q118.612 278.714 114.978 278.714 M114.978 275.01 Q120.788 275.01 123.844 279.617 Q126.922 284.2 126.922 292.95 Q126.922 301.677 123.844 306.283 Q120.788 310.867 114.978 310.867 Q109.168 310.867 106.089 306.283 Q103.034 301.677 103.034 292.95 Q103.034 284.2 106.089 279.617 Q109.168 275.01 114.978 275.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M163.311 284.733 L152.732 295.357 L163.311 305.936 L160.556 308.737 L149.931 298.112 L139.306 308.737 L136.575 305.936 L147.131 295.357 L136.575 284.733 L139.306 281.932 L149.931 292.557 L160.556 281.932 L163.311 284.733 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M175.672 306.26 L183.311 306.26 L183.311 279.895 L175.001 281.561 L175.001 277.302 L183.265 275.635 L187.94 275.635 L187.94 306.26 L195.579 306.26 L195.579 310.195 L175.672 310.195 L175.672 306.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M215.024 278.714 Q211.413 278.714 209.584 282.279 Q207.778 285.82 207.778 292.95 Q207.778 300.057 209.584 303.621 Q211.413 307.163 215.024 307.163 Q218.658 307.163 220.463 303.621 Q222.292 300.057 222.292 292.95 Q222.292 285.82 220.463 282.279 Q218.658 278.714 215.024 278.714 M215.024 275.01 Q220.834 275.01 223.889 279.617 Q226.968 284.2 226.968 292.95 Q226.968 301.677 223.889 306.283 Q220.834 310.867 215.024 310.867 Q209.214 310.867 206.135 306.283 Q203.079 301.677 203.079 292.95 Q203.079 284.2 206.135 279.617 Q209.214 275.01 215.024 275.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M226.968 269.112 L251.08 269.112 L251.08 272.309 L226.968 272.309 L226.968 269.112 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M267.875 267.231 Q265.317 267.231 263.812 268.98 Q262.327 270.729 262.327 273.776 Q262.327 276.804 263.812 278.572 Q265.317 280.321 267.875 280.321 Q270.433 280.321 271.919 278.572 Q273.423 276.804 273.423 273.776 Q273.423 270.729 271.919 268.98 Q270.433 267.231 267.875 267.231 M275.417 255.326 L275.417 258.786 Q273.987 258.109 272.52 257.752 Q271.072 257.394 269.643 257.394 Q265.881 257.394 263.888 259.933 Q261.913 262.473 261.631 267.607 Q262.74 265.971 264.414 265.106 Q266.088 264.222 268.101 264.222 Q272.332 264.222 274.777 266.798 Q277.241 269.356 277.241 273.776 Q277.241 278.102 274.683 280.716 Q272.126 283.33 267.875 283.33 Q263.004 283.33 260.427 279.606 Q257.85 275.864 257.85 268.773 Q257.85 262.115 261.01 258.166 Q264.17 254.197 269.492 254.197 Q270.922 254.197 272.37 254.479 Q273.837 254.761 275.417 255.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M59.793 90.695 L78.1494 90.695 L78.1494 94.6302 L64.0754 94.6302 L64.0754 103.102 Q65.0939 102.755 66.1124 102.593 Q67.131 102.408 68.1495 102.408 Q73.9365 102.408 77.3161 105.579 Q80.6957 108.75 80.6957 114.167 Q80.6957 119.746 77.2235 122.848 Q73.7513 125.926 67.4319 125.926 Q65.256 125.926 62.9875 125.556 Q60.7421 125.186 58.3347 124.445 L58.3347 119.746 Q60.418 120.88 62.6402 121.436 Q64.8625 121.991 67.3393 121.991 Q71.3439 121.991 73.6819 119.885 Q76.0198 117.778 76.0198 114.167 Q76.0198 110.556 73.6819 108.45 Q71.3439 106.343 67.3393 106.343 Q65.4643 106.343 63.5893 106.76 Q61.7375 107.176 59.793 108.056 L59.793 90.695 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M89.9086 119.375 L94.7928 119.375 L94.7928 125.255 L89.9086 125.255 L89.9086 119.375 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M114.978 93.7737 Q111.367 93.7737 109.538 97.3385 Q107.733 100.88 107.733 108.01 Q107.733 115.116 109.538 118.681 Q111.367 122.223 114.978 122.223 Q118.612 122.223 120.418 118.681 Q122.246 115.116 122.246 108.01 Q122.246 100.88 120.418 97.3385 Q118.612 93.7737 114.978 93.7737 M114.978 90.07 Q120.788 90.07 123.844 94.6765 Q126.922 99.2598 126.922 108.01 Q126.922 116.737 123.844 121.343 Q120.788 125.926 114.978 125.926 Q109.168 125.926 106.089 121.343 Q103.034 116.737 103.034 108.01 Q103.034 99.2598 106.089 94.6765 Q109.168 90.07 114.978 90.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M163.311 99.7922 L152.732 110.417 L163.311 120.996 L160.556 123.797 L149.931 113.172 L139.306 123.797 L136.575 120.996 L147.131 110.417 L136.575 99.7922 L139.306 96.9913 L149.931 107.616 L160.556 96.9913 L163.311 99.7922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M175.672 121.32 L183.311 121.32 L183.311 94.9543 L175.001 96.6209 L175.001 92.3617 L183.265 90.695 L187.94 90.695 L187.94 121.32 L195.579 121.32 L195.579 125.255 L175.672 125.255 L175.672 121.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M215.024 93.7737 Q211.413 93.7737 209.584 97.3385 Q207.778 100.88 207.778 108.01 Q207.778 115.116 209.584 118.681 Q211.413 122.223 215.024 122.223 Q218.658 122.223 220.463 118.681 Q222.292 115.116 222.292 108.01 Q222.292 100.88 220.463 97.3385 Q218.658 93.7737 215.024 93.7737 M215.024 90.07 Q220.834 90.07 223.889 94.6765 Q226.968 99.2598 226.968 108.01 Q226.968 116.737 223.889 121.343 Q220.834 125.926 215.024 125.926 Q209.214 125.926 206.135 121.343 Q203.079 116.737 203.079 108.01 Q203.079 99.2598 206.135 94.6765 Q209.214 90.07 215.024 90.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M226.968 84.1714 L251.08 84.1714 L251.08 87.3687 L226.968 87.3687 L226.968 84.1714 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M267.875 82.2906 Q265.317 82.2906 263.812 84.0397 Q262.327 85.7889 262.327 88.8357 Q262.327 91.8638 263.812 93.6317 Q265.317 95.3808 267.875 95.3808 Q270.433 95.3808 271.919 93.6317 Q273.423 91.8638 273.423 88.8357 Q273.423 85.7889 271.919 84.0397 Q270.433 82.2906 267.875 82.2906 M275.417 70.3853 L275.417 73.8459 Q273.987 73.1688 272.52 72.8115 Q271.072 72.4541 269.643 72.4541 Q265.881 72.4541 263.888 74.9932 Q261.913 77.5322 261.631 82.6668 Q262.74 81.0305 264.414 80.1653 Q266.088 79.2814 268.101 79.2814 Q272.332 79.2814 274.777 81.858 Q277.241 84.4159 277.241 88.8357 Q277.241 93.1615 274.683 95.7758 Q272.126 98.3901 267.875 98.3901 Q263.004 98.3901 260.427 94.6661 Q257.85 90.9234 257.85 83.8328 Q257.85 77.1749 261.01 73.2253 Q264.17 69.2568 269.492 69.2568 Q270.922 69.2568 272.37 69.5389 Q273.837 69.8211 275.417 70.3853 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip012)\" d=\"\nM319.511 1025.29 L323.461 1025.15 L327.411 1025 L363.936 1022.27 L400.461 1014.84 L422.411 1005.89 L444.361 991.351 L462.919 972.925 L481.477 947.102 L491.557 929.355 \n  L501.637 908.641 L511.717 884.72 L521.797 857.388 L531.089 829.042 L540.382 797.612 L549.675 763.101 L558.967 725.582 L578.893 635.842 L598.818 536.046 L621.904 414.34 \n  L644.99 296.064 L655.248 247.754 L665.505 203.598 L675.763 164.636 L686.021 131.82 L695.017 108.757 L704.012 91.5147 L713.007 80.4402 L722.002 75.7582 L731.81 78.0465 \n  L741.617 87.989 L751.424 105.345 L761.231 129.7 L771.097 160.68 L780.962 197.436 L790.828 239.134 L800.693 284.859 L822.509 395.457 L844.324 510.718 L865.08 616.37 \n  L885.837 712.023 L896.628 756.48 L907.419 796.916 L918.21 833.199 L929.001 865.333 L947.169 910.418 L965.336 945.163 L987.517 975.731 L1009.7 996.053 L1030.55 1008.32 \n  L1051.4 1015.99 L1084.8 1022.35 L1118.21 1024.89 L1123.35 1025.11 L1128.49 1025.29 L1128.49 1026.23 L1123.35 1026.23 L1118.21 1026.23 L1084.8 1026.23 L1051.4 1026.23 \n  L1030.55 1026.23 L1009.7 1026.23 L987.517 1026.23 L965.336 1026.23 L947.169 1026.23 L929.001 1026.23 L918.21 1026.23 L907.419 1026.23 L896.628 1026.23 L885.837 1026.23 \n  L865.08 1026.23 L844.324 1026.23 L822.509 1026.23 L800.693 1026.23 L790.828 1026.23 L780.962 1026.23 L771.097 1026.23 L761.231 1026.23 L751.424 1026.23 L741.617 1026.23 \n  L731.81 1026.23 L722.002 1026.23 L713.007 1026.23 L704.012 1026.23 L695.017 1026.23 L686.021 1026.23 L675.763 1026.23 L665.505 1026.23 L655.248 1026.23 L644.99 1026.23 \n  L621.904 1026.23 L598.818 1026.23 L578.893 1026.23 L558.967 1026.23 L549.675 1026.23 L540.382 1026.23 L531.089 1026.23 L521.797 1026.23 L511.717 1026.23 L501.637 1026.23 \n  L491.557 1026.23 L481.477 1026.23 L462.919 1026.23 L444.361 1026.23 L422.411 1026.23 L400.461 1026.23 L363.936 1026.23 L327.411 1026.23 L323.461 1026.23 L319.511 1026.23 \n   Z\n  \" fill=\"#ffa500\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n<polyline clip-path=\"url(#clip012)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  319.511,1025.29 323.461,1025.15 327.411,1025 363.936,1022.27 400.461,1014.84 422.411,1005.89 444.361,991.351 462.919,972.925 481.477,947.102 491.557,929.355 \n  501.637,908.641 511.717,884.72 521.797,857.388 531.089,829.042 540.382,797.612 549.675,763.101 558.967,725.582 578.893,635.842 598.818,536.046 621.904,414.34 \n  644.99,296.064 655.248,247.754 665.505,203.598 675.763,164.636 686.021,131.82 695.017,108.757 704.012,91.5147 713.007,80.4402 722.002,75.7582 731.81,78.0465 \n  741.617,87.989 751.424,105.345 761.231,129.7 771.097,160.68 780.962,197.436 790.828,239.134 800.693,284.859 822.509,395.457 844.324,510.718 865.08,616.37 \n  885.837,712.023 896.628,756.48 907.419,796.916 918.21,833.199 929.001,865.333 947.169,910.418 965.336,945.163 987.517,975.731 1009.7,996.053 1030.55,1008.32 \n  1051.4,1015.99 1084.8,1022.35 1118.21,1024.89 1123.35,1025.11 1128.49,1025.29 \n  \"/>\n<path clip-path=\"url(#clip010)\" d=\"\nM976.462 184.507 L1124.17 184.507 L1124.17 80.8274 L976.462 80.8274  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  976.462,184.507 1124.17,184.507 1124.17,80.8274 976.462,80.8274 976.462,184.507 \n  \"/>\n<path clip-path=\"url(#clip010)\" d=\"\nM985.99 153.403 L1043.16 153.403 L1043.16 111.931 L985.99 111.931 L985.99 153.403  Z\n  \" fill=\"#ffa500\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n<polyline clip-path=\"url(#clip010)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  985.99,111.931 1043.16,111.931 \n  \"/>\n<path clip-path=\"url(#clip010)\" d=\"M1066.53 152.355 Q1064.72 156.984 1063.01 158.396 Q1061.3 159.808 1058.43 159.808 L1055.02 159.808 L1055.02 156.244 L1057.52 156.244 Q1059.28 156.244 1060.26 155.41 Q1061.23 154.577 1062.41 151.475 L1063.17 149.531 L1052.69 124.022 L1057.2 124.022 L1065.3 144.299 L1073.4 124.022 L1077.92 124.022 L1066.53 152.355 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip010)\" d=\"M1085.21 146.012 L1092.85 146.012 L1092.85 119.647 L1084.54 121.313 L1084.54 117.054 L1092.8 115.387 L1097.48 115.387 L1097.48 146.012 L1105.12 146.012 L1105.12 149.947 L1085.21 149.947 L1085.21 146.012 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0L5_CrJqYro"
      },
      "source": [
        "For this model to predict the constant mean, we have to use the predict_mean method. \n",
        "\n",
        "With predict mean we do not get the distrution's standard deviation but only the mean. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8MF9gTppUAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6942d07-b440-49f9-beb7-58277fa51a43"
      },
      "source": [
        "ŷ_mean = predict_mean(cmach, rows=test) # On the test set\n",
        "ŷ_mean[1:3] |> pprint"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[179752.7065750736, 179752.7065750736, 179752.7065750736]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqVDVAl9qmzg"
      },
      "source": [
        "As expected, the predicted values is the mean of the normal distribution for all input elements. \n",
        "\n",
        "With our mean predictions, we can calculate a loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8T2hyKHqlai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf1dfad-ffa4-4f31-a4b8-b6261fd4c1cf"
      },
      "source": [
        "rms(ŷ_mean, y[test])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74431.17965544928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YM2v3GPre31"
      },
      "source": [
        "Let's see if we can reduce that error!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Iw979JvXa3"
      },
      "source": [
        "## KNN and Linear regression with l2 reg\n",
        "\n",
        "Now we can use some algorithms. We will use a k nearest neighbor regressor as well as a linear regression with regularization. \n",
        "\n",
        "The nice thing ith MLJ is that when I say we will use 2 models, I do not mean separately, but together. In fact, we will perform a simple stacking of the two algorithms. \n",
        "\n",
        "In addition to this, we will also One hot encode the categorical variables and take the log of the target. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eywTeu3rQNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9baf6c6e-ea29-419c-d43f-4170535cfffe"
      },
      "source": [
        "RidgeRegressor = @load RidgeRegressor pkg=\"MultivariateStats\"\n",
        "KNNRegressor = @load KNNRegressor"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import MLJMultivariateStatsInterface ✔\n",
            "import NearestNeighborModels ✔"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: For silent loading, specify `verbosity=0`. \n",
            "└ @ Main /root/.julia/packages/MLJModels/5itei/src/loading.jl:168\n",
            "┌ Info: For silent loading, specify `verbosity=0`. \n",
            "└ @ Main /root/.julia/packages/MLJModels/5itei/src/loading.jl:168\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighborModels.KNNRegressor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8XbZ22By4Ld"
      },
      "source": [
        "If we see the schema of the data. We can quickly know which columns would have to be tranformed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XIkyZnny7P2",
        "outputId": "c24fe538-32a4-43db-c748-8b5efadf779d"
      },
      "source": [
        "schema(X)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "┌──────────────┬──────────────────────────────────┬───────────────────┐\n",
              "│\u001b[22m _.names      \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes        \u001b[0m│\n",
              "├──────────────┼──────────────────────────────────┼───────────────────┤\n",
              "│ OverallQual  │ CategoricalValue{Int64, UInt32}  │ OrderedFactor{10} │\n",
              "│ GrLivArea    │ Float64                          │ Continuous        │\n",
              "│ Neighborhood │ CategoricalValue{String, UInt32} │ Multiclass{25}    │\n",
              "│ x1stFlrSF    │ Float64                          │ Continuous        │\n",
              "│ TotalBsmtSF  │ Float64                          │ Continuous        │\n",
              "│ BsmtFinSF1   │ Float64                          │ Continuous        │\n",
              "│ LotArea      │ Float64                          │ Continuous        │\n",
              "│ GarageCars   │ Int64                            │ Count             │\n",
              "│ MSSubClass   │ CategoricalValue{String, UInt32} │ Multiclass{15}    │\n",
              "│ GarageArea   │ Float64                          │ Continuous        │\n",
              "│ YearRemodAdd │ Int64                            │ Count             │\n",
              "│ YearBuilt    │ Int64                            │ Count             │\n",
              "└──────────────┴──────────────────────────────────┴───────────────────┘\n",
              "_.nrows = 1456\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVtOrDJ3zBuo"
      },
      "source": [
        "From schema(), we see that 3 columns will have to be transformed. \n",
        "\n",
        "Those are: \n",
        "- OverallQual\n",
        "- Neighborhood \n",
        "- MSSubClass   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKBjB1Mfv-d2",
        "outputId": "858c2dc6-1631-4dfa-bee2-030a505235f6"
      },
      "source": [
        "Xs = source(X)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mSource @437\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Count}, AbstractVector{Multiclass{25}}, AbstractVector{Multiclass{15}}, AbstractVector{OrderedFactor{10}}}}`"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8T67pETzjqw"
      },
      "source": [
        "The source seems to partition the data into the different classes it has. \n",
        "\n",
        "As we saw by schema, we have 4 classes in our DF, hence, source has an Union of 4 classes. \n",
        "\n",
        "Altough, the cardinality of a categorical column creates an unique class (Multiclass{25}, Multiclass{15})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAKmLeLpxnYh",
        "outputId": "3b9e5b26-1e45-4736-9c4c-337bc14f4f96"
      },
      "source": [
        "ys= source(y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mSource @286\u001b[39m ⏎ `AbstractVector{Continuous}`"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS-4fVkOxADc"
      },
      "source": [
        "hot = machine(OneHotEncoder(), Xs) # a hot one encoder machine  applied to the source Xs\n",
        "\n",
        "W = transform(hot, Xs)\n",
        "z = log(ys);"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSEpa2GJ3zN-"
      },
      "source": [
        "This kind of ressembles a workflow in R's tidymodels. \n",
        "Hot machine is applied to source 437 which is the X. Log transorm is applied to source 286 which is y. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNnxGlZsxAin",
        "outputId": "fd02a1a0-5fd0-4961-c9e4-a4cd2eb0af53"
      },
      "source": [
        "W"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mNode{Machine{OneHotEncoder,…}} @323\u001b[39m\n",
              "  args:\n",
              "    1:\t\u001b[34mSource @437\u001b[39m\n",
              "  formula:\n",
              "    transform(\n",
              "        \u001b[0m\u001b[1m\u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m\u001b[22m, \n",
              "        \u001b[34mSource @437\u001b[39m)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4OWbajwyFPg",
        "outputId": "64b96d76-339b-4966-fa37-e1ab9eff47b1"
      },
      "source": [
        "z"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mNode{Nothing} @349\u001b[39m\n",
              "  args:\n",
              "    1:\t\u001b[34mSource @286\u001b[39m\n",
              "  formula:\n",
              "    #125(\n",
              "        \u001b[34mSource @286\u001b[39m)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1MLiIOH2ujw",
        "outputId": "a6494ad0-0327-42e7-ce2d-de2d2805a97f"
      },
      "source": [
        "# Now we define some models and its parameters\n",
        "ridge = machine(RidgeRegressor(lambda=2.5), W, z) # => ridge machinne \n",
        "knn   = machine(KNNRegressor(K=5), W, z) # we create a machine to hold the params. => Knn machine \n",
        "\n",
        "# We create predictions calling predict \n",
        "ẑ₁ = predict(ridge, W) # for now nodes are separated.\n",
        "ẑ₂ = predict(knn, W)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mNode{Machine{KNNRegressor,…}} @355\u001b[39m\n",
              "  args:\n",
              "    1:\t\u001b[34mNode{Machine{OneHotEncoder,…}} @323\u001b[39m\n",
              "  formula:\n",
              "    predict(\n",
              "        \u001b[0m\u001b[1m\u001b[34mMachine{KNNRegressor,…} @131\u001b[39m\u001b[22m, \n",
              "        transform(\n",
              "            \u001b[0m\u001b[1m\u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m\u001b[22m, \n",
              "            \u001b[34mSource @437\u001b[39m))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgDffwpCDn5K"
      },
      "source": [
        "We know combine the predictions of both models doing a simple addition ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz42STWE4V_t",
        "outputId": "f8fec195-39af-45da-b191-658a1ed172d6"
      },
      "source": [
        "ẑ = 0.3ẑ₁ + 0.7ẑ₂ # Here we make a linear combination of both predictions."
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mNode{Nothing} @557\u001b[39m\n",
              "  args:\n",
              "    1:\t\u001b[34mNode{Nothing} @598\u001b[39m\n",
              "    2:\t\u001b[34mNode{Nothing} @913\u001b[39m\n",
              "  formula:\n",
              "    +(\n",
              "        #129(\n",
              "            predict(\n",
              "                \u001b[0m\u001b[1m\u001b[34mMachine{RidgeRegressor,…} @301\u001b[39m\u001b[22m, \n",
              "                transform(\n",
              "                    \u001b[0m\u001b[1m\u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m\u001b[22m, \n",
              "                    \u001b[34mSource @437\u001b[39m))),\n",
              "        #129(\n",
              "            predict(\n",
              "                \u001b[0m\u001b[1m\u001b[34mMachine{KNNRegressor,…} @131\u001b[39m\u001b[22m, \n",
              "                transform(\n",
              "                    \u001b[0m\u001b[1m\u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m\u001b[22m, \n",
              "                    \u001b[34mSource @437\u001b[39m))))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQKv0z-85nZv"
      },
      "source": [
        "Now ẑ is an stacked model of the KNN and the Ridge regressors. ẑ is composed by 30 % of ẑ₁ (Ridge's prediction) and 70% of ẑ₂ (Knn's prediction).\n",
        "\n",
        "Our stacked model gives a higher weight to the knn regressor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-XlzfDv6SuD"
      },
      "source": [
        "Lastly y has to be exponentiated because we took the log of the target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNp_F_mK5lZN",
        "outputId": "cc4a761a-aab3-40a3-a724-1b2f936ca572"
      },
      "source": [
        "ŷ = exp(ẑ)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[34mNode{Nothing} @070\u001b[39m\n",
              "  args:\n",
              "    1:\t\u001b[34mNode{Nothing} @557\u001b[39m\n",
              "  formula:\n",
              "    #127(\n",
              "        +(\n",
              "            #129(\n",
              "                predict(\n",
              "                    \u001b[0m\u001b[1m\u001b[34mMachine{RidgeRegressor,…} @301\u001b[39m\u001b[22m, \n",
              "                    transform(\n",
              "                        \u001b[0m\u001b[1m\u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m\u001b[22m, \n",
              "                        \u001b[34mSource @437\u001b[39m))),\n",
              "            #129(\n",
              "                predict(\n",
              "                    \u001b[0m\u001b[1m\u001b[34mMachine{KNNRegressor,…} @131\u001b[39m\u001b[22m, \n",
              "                    transform(\n",
              "                        \u001b[0m\u001b[1m\u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m\u001b[22m, \n",
              "                        \u001b[34mSource @437\u001b[39m)))))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9nL2BvF6esF"
      },
      "source": [
        "Now we can train our model: \n",
        "\n",
        "ŷ is our workflow, the machine that holds the transformations in order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axrpnKCV5rBc",
        "outputId": "f62c57bc-6f09-4428-92e2-1e0a0111f7f8"
      },
      "source": [
        "fit!(ŷ, rows=train) # will modify inplace ŷ. y\n",
        "ypreds = ŷ(rows=test) # we get the predictions calling directly ŷ. \n",
        "\n",
        "print(\"trained\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trained"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Not retraining \u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{RidgeRegressor,…} @301\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{KNNRegressor,…} @131\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL-GNvDH7S_m"
      },
      "source": [
        "The Root mean squared error is now at 36435. It is less than half of the constant's loss. Clearly the model learned something. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVbLl15u7EOx",
        "outputId": "c8219a7a-8257-4e33-8065-97ed78e1f482"
      },
      "source": [
        "rms(y[test], ypreds)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36435.24190074054"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6cEjXzg8g3l"
      },
      "source": [
        "\n",
        "### Does the stacking even improve our results ? \n",
        "\n",
        "We can fit them one by one and check it's error. \n",
        "\n",
        "A nice feature I found is that we don't have to rebuild everything. We can continue our worflow definition the moment before the stacking. \n",
        "\n",
        "Doing so, ẑ₁ is still the prediciton of the ridge regressor, the same goes for ẑ₂ which are the predictions of the KNN regressor. \n",
        "\n",
        "The difference is that now we do not combine the predictions. This results in two models, each of them with its own predictions. With these predictions we calculate the loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT_56B6H7PJ7",
        "outputId": "f03d6b0b-642b-4936-f4d8-7a8517072075"
      },
      "source": [
        "# Exponentiate the predictions one by one. \n",
        "ŷ_1 = exp(ẑ₁) # machines are already specified until this point\n",
        "ŷ_2 = exp(ẑ₂)\n",
        "\n",
        "# Ridge fitted\n",
        "fit!(ŷ_1, rows=train)\n",
        "y_1_preds = ŷ_1(rows=test) \n",
        "\n",
        "# KNN fitted\n",
        "fit!(ŷ_2, rows=train)\n",
        "y_2_preds = ŷ_2(rows=test) \n",
        " \n",
        "print(\"trained \\n\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trained \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Not retraining \u001b[34mMachine{OneHotEncoder,…} @138\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{RidgeRegressor,…} @411\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{KNNRegressor,…} @683\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrne6Xd68CIS",
        "outputId": "8865900c-7993-4bdb-94e9-b54a5c5ba172"
      },
      "source": [
        "# RIDGE LOSS CALCULATED \n",
        "print(\"RIDGE LOSS: \\n\")\n",
        "rms(y[test],y_1_preds )"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RIDGE LOSS: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23801.6273543209"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCNBbptS9a_U",
        "outputId": "25eb069a-4fb9-47d9-b9ce-36778fb10de1"
      },
      "source": [
        "print(\"KNN LOSS: \\n\")\n",
        "rms(y[test],y_2_preds)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN LOSS: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46134.08868874563"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STuSKblv-HDo"
      },
      "source": [
        "The ridge regression perform worst than the linear regression. Since our linear combinations gives more weigth to the knn's predictions, the loss is higher than the ridge's loss. \n",
        "\n",
        "**We can even review our stacking combination**:\n",
        " \n",
        "Continue development at the point just before the stacking (as we just did). We create a new z prediciton value that is the linear combination of the predicitons of both models. The difference between ẑ_higher_ridge_weigth and ẑ is that we inversed the weigths, **now we give more weigth to the Ridge regressor (70% of ẑ₁)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPKEqL12-DFi",
        "outputId": "f0b5de97-aea6-408e-d707-fb400e3f9093"
      },
      "source": [
        "ẑ_higher_ridge_weigth = 0.7ẑ₁ + 0.3ẑ₂ # Here we make a linear combination of both predictions.\n",
        "\n",
        "# Exponentiate the predictions one by one. \n",
        "ŷ_higher_ridge_weigth = exp(ẑ_higher_ridge_weigth) # machines are already specified until this point\n",
        "\n",
        "# Ridge fitted\n",
        "fit!(ŷ_higher_ridge_weigth, rows=train)\n",
        "ŷ_higher_ridge_weigth_preds = ŷ_higher_ridge_weigth(rows=test) \n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOSS STACK WITH HIGHER WEIGTH FOR RIDGE: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Not retraining \u001b[34mMachine{OneHotEncoder,…} @826\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{RidgeRegressor,…} @814\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n",
            "┌ Info: Not retraining \u001b[34mMachine{KNNRegressor,…} @683\u001b[39m. Use `force=true` to force.\n",
            "└ @ MLJBase /root/.julia/packages/MLJBase/De8Dv/src/machines.jl:357\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26573.498229072895"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv48wna3FlJB",
        "outputId": "ff47a868-d7e3-43cb-d334-5192387eb43d"
      },
      "source": [
        "print(\"LOSS STACK WITH HIGHER WEIGTH FOR RIDGE: \\n\")\n",
        "rms(y[test],ŷ_higher_ridge_weigth_preds)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOSS STACK WITH HIGHER WEIGTH FOR RIDGE: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26573.498229072895"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls8c0yEf_4gG"
      },
      "source": [
        "Our stacked error is still higher than the single model error that the ridge regressor yielded. \n",
        "\n",
        "For now, the ridge regression is the best out of all the models and combinations we tried. \n",
        "\n",
        "In the next notebook we will try parameter tuning and see whether this change our results. In any case, stacking will make sense if models get specialized in specific predictions (if the errors are independent, we could say that they predict based on different parameters or detect different patterns or relations in the data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahaWMsXXCZGj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}